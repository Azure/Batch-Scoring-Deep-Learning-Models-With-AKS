{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Style Transfer on AKS\n",
    "\n",
    "We've tested locally in the previous notebook. Now use an AKS cluster and test that our neural style transfer script still works as expected when running across multiple nodes in parallel on AKS.\n",
    "\n",
    "1. Build AKS Docker Image\n",
    "2. Test style transfer on Docker locally\n",
    "3. Push docker image to Docker hub\n",
    "4. Provision AKS cluster \n",
    "5. Test style transfer on parallel on AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and load .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import set_key, get_key, find_dotenv, load_dotenv\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build AKS Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aks/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile aks/requirements.txt\n",
    "azure==4.0.0\n",
    "torch==0.4.1\n",
    "torchvision==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing aks/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile aks/Dockerfile\n",
    "\n",
    "FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\n",
    "\n",
    "RUN echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/nvidia-ml.list\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        ca-certificates \\\n",
    "        cmake \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        nginx \\\n",
    "        supervisor \\\n",
    "        wget && \\\n",
    "        rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "ENV PYTHON_VERSION=3.6\n",
    "RUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \\\n",
    "    chmod +x ~/miniconda.sh && \\\n",
    "    ~/miniconda.sh -b -p /opt/conda && \\\n",
    "    rm ~/miniconda.sh && \\\n",
    "    /opt/conda/bin/conda create -y --name py$PYTHON_VERSION python=$PYTHON_VERSION && \\\n",
    "    /opt/conda/bin/conda clean -ya\n",
    "ENV PATH /opt/conda/envs/py$PYTHON_VERSION/bin:$PATH\n",
    "ENV LD_LIBRARY_PATH /opt/conda/envs/py$PYTHON_VERSION/lib:/usr/local/cuda/lib64/:$LD_LIBRARY_PATH\n",
    "ENV PYTHONPATH /code/:$PYTHONPATH\n",
    "\n",
    "RUN mkdir /app\n",
    "WORKDIR /app\n",
    "ADD process_images_from_queue.py /app\n",
    "ADD style_transfer.py /app\n",
    "ADD main.py /app\n",
    "ADD util.py /app\n",
    "ADD requirements.txt /app\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "CMD [\"python\", \"main.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  46.59kB\n",
      "Step 1/17 : FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\n",
      " ---> f4f6aaaaa057\n",
      "Step 2/17 : RUN echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /\" > /etc/apt/sources.list.d/nvidia-ml.list\n",
      " ---> Using cache\n",
      " ---> 4196af2ba86e\n",
      "Step 3/17 : RUN apt-get update && apt-get install -y --no-install-recommends         build-essential         ca-certificates         cmake         curl         git         nginx         supervisor         wget &&         rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 8ddcde9d280a\n",
      "Step 4/17 : ENV PYTHON_VERSION=3.6\n",
      " ---> Using cache\n",
      " ---> 5a047de1f83a\n",
      "Step 5/17 : RUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh  &&     chmod +x ~/miniconda.sh &&     ~/miniconda.sh -b -p /opt/conda &&     rm ~/miniconda.sh &&     /opt/conda/bin/conda create -y --name py$PYTHON_VERSION python=$PYTHON_VERSION &&     /opt/conda/bin/conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 42fd602eabaa\n",
      "Step 6/17 : ENV PATH /opt/conda/envs/py$PYTHON_VERSION/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> e15ad65c7c32\n",
      "Step 7/17 : ENV LD_LIBRARY_PATH /opt/conda/envs/py$PYTHON_VERSION/lib:/usr/local/cuda/lib64/:$LD_LIBRARY_PATH\n",
      " ---> Using cache\n",
      " ---> 2281999563de\n",
      "Step 8/17 : ENV PYTHONPATH /code/:$PYTHONPATH\n",
      " ---> Using cache\n",
      " ---> e73919aaabd9\n",
      "Step 9/17 : RUN mkdir /app\n",
      " ---> Using cache\n",
      " ---> a586b032becf\n",
      "Step 10/17 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 379f3f867f7a\n",
      "Step 11/17 : ADD process_images_from_queue.py /app\n",
      " ---> Using cache\n",
      " ---> e111377dba18\n",
      "Step 12/17 : ADD style_transfer.py /app\n",
      " ---> Using cache\n",
      " ---> 18f30f096b22\n",
      "Step 13/17 : ADD main.py /app\n",
      " ---> Using cache\n",
      " ---> 2375242e751e\n",
      "Step 14/17 : ADD util.py /app\n",
      " ---> Using cache\n",
      " ---> e1d139c06c09\n",
      "Step 15/17 : ADD requirements.txt /app\n",
      " ---> Using cache\n",
      " ---> 4311eafff908\n",
      "Step 16/17 : RUN pip install --no-cache-dir -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 462971dcb4f2\n",
      "Step 17/17 : CMD [\"python\", \"main.py\"]\n",
      " ---> Using cache\n",
      " ---> 5cae96eef026\n",
      "Successfully built 5cae96eef026\n",
      "Successfully tagged batchscoringdl_aks_app:latest\n"
     ]
    }
   ],
   "source": [
    "!sudo docker build -t {get_key(env_path, \"AKS_IMAGE\")} aks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Docker image locally (before deploying on AKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add images to queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_frames_dir = \"orangutan_frames_test\"\n",
    "docker_output_frames_dir = \"orangutan_frames_docker_test_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding images from 'orangutan_frames_test' in storage to queue 'batchscoringdlqueue'\n",
      "2018-12-07 20:37:18,080 [root:add_images_to_queue.py:48] DEBUG - Queue limit is reached. Exiting process...\n"
     ]
    }
   ],
   "source": [
    "!python aci/add_images_to_queue.py \\\n",
    "    --input-dir {input_frames_dir} \\\n",
    "    --output-dir {docker_output_frames_dir} \\\n",
    "    --queue-limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -e \"s/=\\\"/=/g\" -e \"s/\\\"$//g\" .env > .env.docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "strip_out"
    ]
   },
   "outputs": [],
   "source": [
    "!cat .env.docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run docker locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07 20:37:24,568 [root:process_images_from_queue.py:35] DEBUG - Downloading style model from directory models\n",
      "2018-12-07 20:37:25,571 [root:process_images_from_queue.py:50] DEBUG - The following model were downloaded: [\"candy.pth\",\"model.pth\",\"mosaic.pth\",\"rain_princess.pth\",\"udnie.pth\"]\n",
      "2018-12-07 20:37:25,571 [root:process_images_from_queue.py:88] DEBUG - It took 1.07 seconds to download style model.\n",
      "2018-12-07 20:37:25,571 [root:process_images_from_queue.py:91] DEBUG - Start listening to queue 'batchscoringdlqueue' on service bus...\n",
      "2018-12-07 20:37:25,572 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:25,637 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000001_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:25,644 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000001_frame.jpg\n",
      "2018-12-07 20:37:29,345 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000001_frame.jpg\n",
      "2018-12-07 20:37:29,461 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000001_frame.jpg\n",
      "2018-12-07 20:37:29,485 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:29,485 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:29,538 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:29,542 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000002_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:29,578 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000002_frame.jpg\n",
      "2018-12-07 20:37:29,605 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000002_frame.jpg\n",
      "2018-12-07 20:37:29,696 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000002_frame.jpg\n",
      "2018-12-07 20:37:29,723 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:29,723 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:29,758 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:29,762 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000003_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:29,771 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000003_frame.jpg\n",
      "2018-12-07 20:37:29,798 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000003_frame.jpg\n",
      "2018-12-07 20:37:29,886 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000003_frame.jpg\n",
      "2018-12-07 20:37:29,913 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:29,913 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:29,990 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:29,994 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000004_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:30,003 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000004_frame.jpg\n",
      "2018-12-07 20:37:30,031 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000004_frame.jpg\n",
      "2018-12-07 20:37:30,117 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000004_frame.jpg\n",
      "2018-12-07 20:37:30,145 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:30,145 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:30,209 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:30,213 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000005_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:30,221 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000005_frame.jpg\n",
      "2018-12-07 20:37:30,249 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000005_frame.jpg\n",
      "2018-12-07 20:37:30,344 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000005_frame.jpg\n",
      "2018-12-07 20:37:30,371 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:30,372 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:30,474 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:30,479 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000006_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:30,486 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000006_frame.jpg\n",
      "2018-12-07 20:37:30,515 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000006_frame.jpg\n",
      "2018-12-07 20:37:30,603 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000006_frame.jpg\n",
      "2018-12-07 20:37:30,629 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:30,630 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:30,694 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:30,699 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000007_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:30,706 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000007_frame.jpg\n",
      "2018-12-07 20:37:30,731 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000007_frame.jpg\n",
      "2018-12-07 20:37:30,822 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000007_frame.jpg\n",
      "2018-12-07 20:37:30,849 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:30,849 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:30,935 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:30,939 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000008_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:30,946 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000008_frame.jpg\n",
      "2018-12-07 20:37:30,973 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000008_frame.jpg\n",
      "2018-12-07 20:37:31,058 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000008_frame.jpg\n",
      "2018-12-07 20:37:31,087 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:31,087 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:31,180 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:31,184 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000009_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:31,193 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000009_frame.jpg\n",
      "2018-12-07 20:37:31,220 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000009_frame.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07 20:37:31,313 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000009_frame.jpg\n",
      "2018-12-07 20:37:31,339 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:31,339 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:31,428 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:37:31,433 [root:process_images_from_queue.py:123] DEBUG - Queue message body: {'input_frame': '000010_frame.jpg', 'input_dir': 'orangutan_frames_test', 'output_dir': 'orangutan_frames_docker_test_processed'}\n",
      "2018-12-07 20:37:31,439 [root:process_images_from_queue.py:140] DEBUG - Starting style transfer on orangutan_frames_test/000010_frame.jpg\n",
      "2018-12-07 20:37:31,467 [root:style_transfer.py:157] DEBUG - Processing .aks/input/000010_frame.jpg\n",
      "2018-12-07 20:37:31,560 [root:process_images_from_queue.py:150] DEBUG - Finished style transfer on orangutan_frames_test/000010_frame.jpg\n",
      "2018-12-07 20:37:31,591 [root:process_images_from_queue.py:162] DEBUG - Uploaded output file and log file to storage\n",
      "2018-12-07 20:37:31,592 [root:process_images_from_queue.py:171] DEBUG - Deleting queue message...\n",
      "2018-12-07 20:37:31,708 [root:process_images_from_queue.py:96] DEBUG - Peek queue...\n",
      "2018-12-07 20:38:01,716 [root:process_images_from_queue.py:102] DEBUG - Receiver has timed out, queue is empty. Exiting program...\n",
      "/opt/conda/envs/py3.6/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "!sudo docker run --runtime=nvidia -e TERMINATE=True --env-file \".env.docker\" {get_key(env_path, \"AKS_IMAGE\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that queue is now empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!az servicebus queue show \\\n",
    "    --name {get_key(env_path, \"SB_QUEUE\")} \\\n",
    "    --namespace-name {get_key(env_path, \"SB_NAMESPACE\")} \\\n",
    "    --resource-group {get_key(env_path, \"RESOURCE_GROUP\")} \\\n",
    "    --query 'countDetails.activeMessageCount'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the queue is emptied, you can use storage explorer to check that all the output images are correctly saved in the directory `orangutan_frames_docker_test_processed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker tag {get_key(env_path, \"AKS_IMAGE\")} {get_key(env_path, \"DOCKER_LOGIN\")}/{get_key(env_path, \"AKS_IMAGE\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/jiata/batchscoringdl_aks_app]\n",
      "\n",
      "\u001b[1Bac7ea077: Preparing \n",
      "\u001b[1Bfa35397b: Preparing \n",
      "\u001b[1B7b7ed3d8: Preparing \n",
      "\u001b[1B474baca9: Preparing \n",
      "\u001b[1B7d7c7fa2: Preparing \n",
      "\u001b[1B5d58a915: Preparing \n",
      "\u001b[1B8452f77e: Preparing \n",
      "\u001b[1Baad0d176: Preparing \n",
      "\u001b[1Bff05626e: Preparing \n",
      "\u001b[1B9048222b: Preparing \n",
      "\u001b[1Bf7dc85a1: Preparing \n",
      "\u001b[1B2df89268: Preparing \n",
      "\u001b[1Bd8f0884d: Preparing \n",
      "\u001b[1B87fdb58c: Preparing \n",
      "\u001b[1B8fb03d12: Preparing \n",
      "\u001b[1B843615e2: Preparing \n",
      "\u001b[12Bd58a915: Waiting g \n",
      "\u001b[1B9c0f8a0b: Preparing \n",
      "\u001b[13B452f77e: Waiting g \n",
      "\u001b[1B91f0ffec: Layer already exists \u001b[19A\u001b[1K\u001b[K\u001b[20A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[Klatest: digest: sha256:4ad2d4c94c9a4ec7994590b436684f3585084c02686d430194260b42f0129520 size: 4507\n"
     ]
    }
   ],
   "source": [
    "!sudo docker push {get_key(env_path, \"DOCKER_LOGIN\")}/{get_key(env_path, \"AKS_IMAGE\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set how many nodes you want to provision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_count = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are enough core of the \"Standard_NC6s_v3\". If not, check that there are enough core of the \"Standard_D2s_v3\". If not, raise exception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking quota for family size NCSv3...\n",
      "There are enough cores, you may continue...\n"
     ]
    }
   ],
   "source": [
    "vm_dict = {\n",
    "    \"NCSv3\": {\n",
    "        \"size\": \"Standard_NC6s_v3\",\n",
    "        \"cores\": 6\n",
    "    },\n",
    "    \"DSv3\": {\n",
    "        \"size\": \"Standard_D2s_v3\",\n",
    "        \"cores\": 2\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Checking quota for family size NCSv3...\")\n",
    "vm_family = \"NCSv3\"\n",
    "requested_cores = node_count * vm_dict[vm_family][\"cores\"]\n",
    "\n",
    "def check_quota(vm_family):\n",
    "    \"\"\"\n",
    "    returns quota object\n",
    "    \"\"\"\n",
    "    results = subprocess.run([\n",
    "        \"az\", \"vm\", \"list-usage\", \n",
    "        \"--location\", get_key(env_path, \"REGION\"), \n",
    "        \"--query\", \"[?contains(localName, '%s')].{max:limit, current:currentValue}\" % (vm_family)\n",
    "    ], stdout=subprocess.PIPE)\n",
    "    quota = json.loads(''.join(results.stdout.decode('utf-8')))\n",
    "    return int(quota[0]['max']) - int(quota[0]['current'])\n",
    "\n",
    "diff = check_quota(vm_family)\n",
    "if diff <= requested_cores:\n",
    "    print(\"Not enough cores of NCSv3 in region, asking for {} but have {}\".format(requested_cores, diff))\n",
    "    \n",
    "    print(\"Retrying with family size DSv3...\")\n",
    "    vm_family = \"DSv3\"\n",
    "    requested_cores = node_count * vm_dict[vm_family][\"cores\"]\n",
    "    \n",
    "    diff = check_quota(vm_family)\n",
    "    if diff <= requested_cores:\n",
    "        print(\"Not enough cores of DSv3 in region, asking for {} but have {}\".format(requested_cores, diff))\n",
    "        raise Exception(\"Core Limit\", \"Note enough cores to satisfy request\")\n",
    "\n",
    "print(\"There are enough cores, you may continue...\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the aks cluster. This step may take a while... Please note that this step creates another resource group in your subscription containing the actual compute of the AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "strip_out"
    ]
   },
   "outputs": [],
   "source": [
    "!az aks create \\\n",
    "    --resource-group {get_key(env_path, \"RESOURCE_GROUP\")} \\\n",
    "    --name {get_key(env_path, \"AKS_CLUSTER\")} \\\n",
    "    --node-count {node_count} \\\n",
    "    --node-vm-size {vm_dict[vm_family][\"size\"]} \\\n",
    "    --generate-ssh-keys \\\n",
    "    --service-principal {get_key(env_path, \"SP_CLIENT\")} \\\n",
    "    --client-secret {get_key(env_path, \"SP_SECRET\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Kubectl - this tool is used to manage the kubernetes cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading client to \"/usr/local/bin/kubectl\" from \"https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubectl\"\u001b[0m\n",
      "\u001b[33mPlease ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo az aks install-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"batchscoringdlcluster\" as current context in /home/jiata/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials \\\n",
    "    --resource-group {get_key(env_path, 'RESOURCE_GROUP')}\\\n",
    "    --name {get_key(env_path, 'AKS_CLUSTER')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that our nodes are up and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS   ROLES   AGE     VERSION\r\n",
      "aks-nodepool1-40264992-0   Ready    agent   4m40s   v1.9.11\r\n",
      "aks-nodepool1-40264992-1   Ready    agent   4m26s   v1.9.11\r\n",
      "aks-nodepool1-40264992-2   Ready    agent   4m36s   v1.9.11\r\n",
      "aks-nodepool1-40264992-3   Ready    agent   4m32s   v1.9.11\r\n",
      "aks-nodepool1-40264992-4   Ready    agent   4m31s   v1.9.11\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy docker image to AKS cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy our neural style transfer script into our AKS cluster, we need to define what the deployment will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_deployment_json = {\n",
    "    \"apiVersion\": \"apps/v1beta1\",\n",
    "    \"kind\": \"Deployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": \"aks-app\", \n",
    "        \"labels\": {\n",
    "            \"purpose\": \"dequeue_messages_and_apply_style_transfer\"\n",
    "        }\n",
    "    },\n",
    "    \"spec\": {\n",
    "        \"replicas\": node_count,\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"labels\": {\n",
    "                    \"app\": \"aks-app\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"name\": \"aks-app\",\n",
    "                        \"image\": \"{}/{}:latest\".format(get_key(env_path, \"DOCKER_LOGIN\"), get_key(env_path, \"AKS_IMAGE\")),\n",
    "                        \"volumeMounts\": [\n",
    "                            {\n",
    "                                \"mountPath\": \"/usr/local/nvidia\", \n",
    "                                \"name\": \"nvidia\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"resources\": {\n",
    "                            \"requests\": {\n",
    "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                            },\n",
    "                            \"limits\": {\n",
    "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                            },\n",
    "                        },\n",
    "                        \"ports\": [{\n",
    "                            \"containerPort\": 433\n",
    "                        }],\n",
    "                        \"env\": [\n",
    "                            {\n",
    "                                \"name\": \"LB_LIBRARY_PATH\",\n",
    "                                \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.6/lib\",\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"DP_DISABLE_HEALTHCHECKS\", \n",
    "                                \"value\": \"xids\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"STORAGE_MODEL_DIR\",\n",
    "                                \"value\": get_key(env_path, \"STORAGE_MODEL_DIR\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"SUBSCRIPTION_ID\",\n",
    "                                \"value\": get_key(env_path, \"SUBSCRIPTION_ID\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"RESOURCE_GROUP\",\n",
    "                                \"value\": get_key(env_path, \"RESOURCE_GROUP\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"REGION\",\n",
    "                                \"value\": get_key(env_path, \"REGION\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"STORAGE_ACCOUNT_NAME\", \n",
    "                                \"value\": get_key(env_path, \"STORAGE_ACCOUNT_NAME\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"STORAGE_ACCOUNT_KEY\",\n",
    "                                \"value\": get_key(env_path, \"STORAGE_ACCOUNT_KEY\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"STORAGE_CONTAINER_NAME\",\n",
    "                                \"value\": get_key(env_path, \"STORAGE_CONTAINER_NAME\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"SB_SHARED_ACCESS_KEY_NAME\",\n",
    "                                \"value\": get_key(env_path, \"SB_SHARED_ACCESS_KEY_NAME\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"SB_SHARED_ACCESS_KEY_VALUE\",\n",
    "                                \"value\": get_key(env_path, \"SB_SHARED_ACCESS_KEY_VALUE\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"SB_NAMESPACE\",\n",
    "                                \"value\": get_key(env_path, \"SB_NAMESPACE\")\n",
    "                            },\n",
    "                            {\n",
    "                                \"name\": \"SB_QUEUE\", \n",
    "                                \"value\": get_key(env_path, \"SB_QUEUE\")\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                \"volumes\": [\n",
    "                    {\n",
    "                        \"name\": \"nvidia\", \n",
    "                        \"hostPath\": {\n",
    "                            \"path\": \"/usr/local/nvidia\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"aks_deployment.json\", \"w\") as outfile:\n",
    "    json.dump(aks_deployment_json, outfile, indent=4, sort_keys=True)\n",
    "    outfile.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run style transfer on AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 100 new messages to the queue so that we can use our newly created AKS cluster to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_output_frames_dir = \"orangutan_frames_aks_test_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding images from 'orangutan_frames_test' in storage to queue 'batchscoringdlqueue'\n",
      "2018-12-07 20:50:20,196 [root:add_images_to_queue.py:48] DEBUG - Queue limit is reached. Exiting process...\n"
     ]
    }
   ],
   "source": [
    "!python aci/add_images_to_queue.py \\\n",
    "    --input-dir {input_frames_dir} \\\n",
    "    --output-dir {aks_output_frames_dir} \\\n",
    "    --queue-limit 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `aks_deployment.json` we created, create our deployment on AKS. This can take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/aks-app created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f aks_deployment.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that its been deployed, check our pods to make sure that the deployment worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       READY   STATUS              RESTARTS   AGE\r\n",
      "aks-app-864c65b9bb-46dnd   0/1     ContainerCreating   0          4m\r\n",
      "aks-app-864c65b9bb-d54cc   0/1     ContainerCreating   0          4m\r\n",
      "aks-app-864c65b9bb-d575x   0/1     ContainerCreating   0          4m\r\n",
      "aks-app-864c65b9bb-dtd6x   0/1     ContainerCreating   0          4m\r\n",
      "aks-app-864c65b9bb-nj8jh   0/1     ContainerCreating   0          4m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the logs of one of the pods to inspect the process running inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "strip_out"
    ]
   },
   "outputs": [],
   "source": [
    "pod_json = !kubectl get pods -o json\n",
    "pod_dict = json.loads(''.join(pod_json))\n",
    "!kubectl logs {pod_dict['items'][0]['metadata']['name']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that there are now no more messages in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!az servicebus queue show \\\n",
    "    --name {get_key(env_path, \"SB_QUEUE\")} \\\n",
    "    --namespace-name {get_key(env_path, \"SB_NAMESPACE\")} \\\n",
    "    --resource-group {get_key(env_path, \"RESOURCE_GROUP\")} \\\n",
    "    --query 'countDetails.activeMessageCount'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the queue is emptied, you can use storage explorer to check that all the output images are correctly saved in the directory `orangutan_frames_aks_test_processed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor in kubernetes dashboard\n",
    "You can use the Kubernetes dashboard to monitor the cluster using the following commands:\n",
    "\n",
    "```\n",
    "# use the kube_dashboard_access.yaml to create a deployment\n",
    "!kubectl create -f kube_dashboard_access.yaml\n",
    "\n",
    "# use this command to browse\n",
    "!az aks browse -n {get_key(env_path, \"AKS_CLUSTER\")} -g {get_key(env_path, \"RESOURCE_GROUP\")}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional commands for AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale your AKS cluster:\n",
    "\n",
    "```\n",
    "!az aks scale \\\n",
    "    --name {get_key(env_path, \"AKS_CLUSTER\")} \\\n",
    "    --resource-group {get_key(env_path, \"RESOURCE_GROUP\")} \\\n",
    "    --node-count 10\n",
    "```\n",
    "\n",
    "Scale your deployment:\n",
    "```\n",
    "!kubectl scale deployment.apps/aks-app --replicas=10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to the next [notebook](/notebooks/04_deploy_logic_app.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:batchscoringdl]",
   "language": "python",
   "name": "conda-env-batchscoringdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
